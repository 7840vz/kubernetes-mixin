rule_files:
  - prometheus_alerts.yaml
  - prometheus_rules.yaml

evaluation_interval: 1m

tests:
- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes{job="kubelet",namespace="monitoring",persistentvolumeclaim="somepvc"}'
    values: '1024 512 64 16'
  - series: 'kubelet_volume_stats_capacity_bytes{job="kubelet",namespace="monitoring",persistentvolumeclaim="somepvc"}'
    values: '1024 1024 1024 1024'
  alert_rule_test:
  - eval_time: 1m
    alertname: KubePersistentVolumeUsageCritical
  - eval_time: 2m
    alertname: KubePersistentVolumeUsageCritical
  - eval_time: 3m
    alertname: KubePersistentVolumeUsageCritical
  - eval_time: 4m
    alertname: KubePersistentVolumeUsageCritical
    exp_alerts:
    - exp_labels:
        job: kubelet
        namespace: monitoring
        persistentvolumeclaim: somepvc
        severity: critical
      exp_annotations:
        message: 'The PersistentVolume claimed by somepvc in Namespace monitoring is only 1.562% free.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical

- interval: 1m
  input_series:
  - series: 'kubelet_volume_stats_available_bytes{job="kubelet",namespace="monitoring",persistentvolumeclaim="somepvc"}'
    values: '1024-10x61'
  - series: 'kubelet_volume_stats_capacity_bytes{job="kubelet",namespace="monitoring",persistentvolumeclaim="somepvc"}'
    values: '32768+0x61'
  alert_rule_test:
  - eval_time: 1h
    alertname: KubePersistentVolumeFullInFourDays
  - eval_time: 61m
    alertname: KubePersistentVolumeFullInFourDays
    exp_alerts:
    - exp_labels:
        job: kubelet
        namespace: monitoring
        persistentvolumeclaim: somepvc
        severity: critical
      exp_annotations:
        message: 'Based on recent sampling, the PersistentVolume claimed by somepvc in Namespace monitoring is expected to fill up within four days. Currently 1.263% is available.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays

- interval: 1m
  input_series:
  - series: 'kubelet_running_pod_count{endpoint="https-metrics",instance="10.0.2.15:10250",job="kubelet",namespace="kube-system",node="minikube",service="kubelet"}'
    values: '110 110 110 110 110 110 110 110 110 110 110 110 110 110 108 108'
  - series: 'kubelet_node_name{endpoint="https-metrics",instance="10.0.2.15:10250",job="kubelet",namespace="kube-system",node="minikube",service="kubelet"}'
    values: '1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'
  - series: 'kube_node_status_capacity_pods{instance="172.17.0.5:8443",node="minikube", job="kube-state-metrics"}'
    values: '110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110'
  alert_rule_test:
  - eval_time: 10m
    alertname: KubeletTooManyPods
  - eval_time: 15m
    alertname: KubeletTooManyPods
    exp_alerts:
    - exp_labels:
        node: minikube
        severity: warning
      exp_annotations:
        message: "Kubelet 'minikube' is running at 98.18% of its Pod capacity."
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods

- interval: 1m
  input_series:
  - series: 'kube_pod_container_resource_requests_cpu_cores{container="kube-apiserver-67",endpoint="https-main",instance="ksm-1",job="kube-state-metrics",namespace="kube-apiserver",node="node-1",pod="pod-1",service="ksm"}'
    values: '0.15+0x10'
  - series: 'kube_pod_container_resource_requests_cpu_cores{container="kube-apiserver-67",endpoint="https-main",instance="ksm-2",job="kube-state-metrics",namespace="kube-apiserver",node="node-1",pod="pod-1",service="ksm"}'
    values: '0.1+0x10'
  - series: 'kube_pod_container_resource_requests_memory_bytes{container="kube-apiserver-67",endpoint="https-main",instance="ksm-1",job="kube-state-metrics",namespace="kube-apiserver",node="node-1",pod="pod-1",service="ksm"}'
    values: '1E9+0x10'
  - series: 'kube_pod_container_resource_requests_memory_bytes{container="kube-apiserver-67",endpoint="https-main",instance="ksm-2",job="kube-state-metrics",namespace="kube-apiserver",node="node-1",pod="pod-1",service="ksm"}'
    values: '0.5E9+0x10'
  # Duplicate kube_pod_status_phase timeseries for the same pod.
  - series: 'kube_pod_status_phase{endpoint="https-main",instance="ksm-1",job="kube-state-metrics",namespace="kube-apiserver",phase="Running",pod="pod-1",service="ksm"}'
    values: '1 stale'
  - series: 'kube_pod_status_phase{endpoint="https-main",instance="ksm-1",job="kube-state-metrics",namespace="kube-apiserver",phase="Pending",pod="pod-1",service="ksm"}'
    values: '1+0x10'
  - series: 'kube_pod_status_phase{endpoint="https-main",instance="ksm-2",job="kube-state-metrics",namespace="kube-apiserver",phase="Running",pod="pod-1",service="ksm"}'
    values: '1+0x10'
  promql_expr_test:
  - eval_time: 0m
    expr: namespace:kube_pod_container_resource_requests_cpu_cores:sum
    exp_samples:
    - value: 0.15
      labels: 'namespace:kube_pod_container_resource_requests_cpu_cores:sum{namespace="kube-apiserver"}'
  - eval_time: 0m
    expr: namespace:kube_pod_container_resource_requests_memory_bytes:sum
    exp_samples:
    - value: 1.0e+9
      labels: 'namespace:kube_pod_container_resource_requests_memory_bytes:sum{namespace="kube-apiserver"}'
  - eval_time: 1m
    expr: namespace:kube_pod_container_resource_requests_cpu_cores:sum
    exp_samples:
    - value: 0.15
      labels: 'namespace:kube_pod_container_resource_requests_cpu_cores:sum{namespace="kube-apiserver"}'
  - eval_time: 1m
    expr: namespace:kube_pod_container_resource_requests_memory_bytes:sum
    exp_samples:
    - value: 1.0e+9
      labels: 'namespace:kube_pod_container_resource_requests_memory_bytes:sum{namespace="kube-apiserver"}'
